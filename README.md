# Supermarket Price Scraper

A Node.js/TypeScript backend service that scrapes and aggregates product prices from major Ukrainian supermarket chains.

## Features

- ğŸ¤– Automated web scraping using Puppeteer
- ğŸª Multi-store support (Silpo, with ATB and others planned)
- ğŸ”„ Configurable scraping intervals
- ğŸ“Š Structured product data extraction
- ğŸ” Price monitoring and tracking
- âš¡ TypeScript for type safety and better development experience

## Project Structure

```
scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # Application entry point
â”‚   â”œâ”€â”€ api/              # API endpoints and handlers
â”‚   â”‚   â””â”€â”€ products.ts   # Product-related API endpoints
â”‚   â””â”€â”€ scrapers/         # Store-specific scrapers
â”‚       â””â”€â”€ silpoScraper.ts  # Silpo supermarket scraper
â”œâ”€â”€ config.ts             # Configuration settings
â”œâ”€â”€ package.json         # Project dependencies
â””â”€â”€ tsconfig.json       # TypeScript configuration
```

## Prerequisites

- Node.js (LTS version recommended)
- npm or yarn
- TypeScript

## Installation

1. Install dependencies:
```bash
npm install
```

## Development

Start the scraper in development mode with hot reloading:
```bash
npm run dev
```

## Building

Build the TypeScript code:
```bash
npm run build
```

## Production

Run the production build:
```bash
npm run start
```

## Scripts

- `npm run dev` - Run in development mode using ts-node
- `npm run build` - Compile TypeScript to JavaScript
- `npm start` - Run the compiled JavaScript

## Dependencies

### Production
- `puppeteer` - Headless Chrome automation library

### Development
- `typescript` - TypeScript language support
- `ts-node` - TypeScript execution engine
- `@types/node` - Node.js type definitions

## Configuration

The scraper can be configured through `config.ts`. Configuration options include:

- Scraping intervals
- Target stores
- Request timeouts
- Rate limiting parameters
- Product categories to scrape

## Adding New Scrapers

1. Create a new scraper file in `src/scrapers/`
2. Implement the scraping logic using Puppeteer
3. Export the scraper interface
4. Register the scraper in the main application

Example structure for a new scraper:

```typescript
import puppeteer from 'puppeteer';

export async function scrapeProducts() {
    const browser = await puppeteer.launch();
    try {
        // Scraping implementation
    } finally {
        await browser.close();
    }
}
```

## Error Handling

The scraper implements several error handling mechanisms:

- Automatic retries for failed requests
- Rate limiting to prevent IP blocking
- Error logging and monitoring
- Graceful shutdown handling

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

ISC
